# Altmetric Attention Score (AAS)

## What it is
The Altmetric Attention Score (AAS) is a weighted count of all of the attention Altmetric has found for a research output in the [sources](https://www.altmetric.com/about-our-data/our-sources/) tracked by Altmetric. These include [mentions](https://help.altmetric.com/support/solutions/folders/6000237990) in public policy documents, mainstream media, online reference managers (Mendeley), post-publication peer-review platforms (Pubpeer and Publons), references in Wikipedia, open syllabus project, patents, blogs, citations (Dimensions and Web of Science where licensed), research highlights, social media (Facebook mentions on curated list of public pages only), Twitter Historical sources, LinkedIn, Google+, Sina Weibo and Pinterest, and multimedia and online platforms (YouTube, Reddit, Q&A stack overflow). Some of these data sources in particular Google+, Weibo, LinkedIn, and Pinterest are not trackable since the years 2019, 2015, 2014, and 2013 orderly but their historical data are kept.

## How it works
The [AAS](https://help.altmetric.com/support/solutions/articles/6000233311-how-is-the-altmetric-attention-score-calculated-) takes into account the volume, sources, and authors of attention received by a research output across the tracked online attention sources. Although the Altmetric article page shows Mendeley readers, Dimensions citation counts, and CiteULike bookmarks, these particular data do not count towards the score. Altmetrics have been incorporated into researchers’ websites, institutional repositories, journal websites and thus it applies to journal articles and any research output deposited to a repository that Altmeric.com tracks (e.g. Figshare or an institutional repository). The AAS can be found in all products offered by [Altmetric](http://www.altmetric.com/), including the free researcher bookmarklet and on many journal publisher websites and repositories (such as [Figshare](https://figshare.com/)). The [Dimensions](https://app.dimensions.ai/discover/publication) database also includes the Altmetric Attention Score for the articles it indexes.

AAS is an aggregated weighted article level indicator that can be used to:
- showcase the overall volume of attention that research has received online
- understand the “Score in Context” (found on Altmetric details pages) and particularly how a research output’s score compares to other scores
- monitor the online attention/outreach received by outputs published/produced by publishers, institutes, research groups, etc.


## What to keep in mind

The idea of alternative level metrics (alt-metrics) was first introduced by [Jason Priem in 2010] (1). Some of the documented limitations are related to individual altmetric scores or the aggregated AAS, while some are related to the underlying data used to calculate the metric. Some commonly noted limitations of the AAS (and altmetrics data in general) are as follows: <br>
- AAS is considered as a composite indicator which lumps together fundamentally different metrics (e.g. Twitter, blogs, views, etc.) with different nature. <br>
- AAS is an aggregated score and thus should not be used in the aggregate form for comparison with non aggregated metrics (citations). Keeping the different altmetric scores as separate entities suggested to be the best choice for transparent approaches in assessments (2). <br>
- AAS does not take into account the sentiments of mentions made about research objects, and thus does not help one understand the positive nor negative attention that a piece of research has received. <br>
- AAS doesn’t reflect publications without unique identifiers (e.g. DOI, PMID, etc.) since these publications are not tracked by Altmetric.com. <br>
- Mentions of scientific publications on social media platforms (such as Twitter) need to include a direct link to the scientific publication to be tracked by Altmetric.com. <br>
- Legitimate self-promotion by authors may artificially increase the AAS (3). <br>
- The AAS should not be used as a direct measure of research impact or quality of any kind. <br>
- The AAS lacks transparency. It is not possible to fully audit the AAS, as the weighting of the score depends upon non-public, company-assigned “tiers” for news sources, Twitter users, and some other sources that mention a research output. <br>
- In general, heterogeneity, dependency, lack of common definition, and data quality are main challenges of altmetric data and indicators (2, 4, 5). <br>
- Differences in coverage and frequency of updates of altmetric aggregators influence the counts of altmetric indicators 6, 7). <br>
<br>

## Learn more

- Banshal S.K., Basu A., Singh V.K., Muhuri P.K. (2018) Scientific vs. Public Attention: A Comparison of Top Cited Papers in WoS and Top Papers by Altmetric Score. In: Erdt M., Sesagiri Raamkumar A., Rasmussen E., Theng YL. (eds) Altmetrics for Research Outputs Measurement and Scholarly Information Management. AROSIM 2018. Communications in Computer and Information Science, vol 856. Springer, Singapore. [https://doi.org/10.1007/978-981-13-1053-9_7](https://doi.org/10.1007/978-981-13-1053-9_7) <br>
- Bar-Ilan, J. & Halevi, G. (2017). Altmetric Counts from Different Sources: A Case Study of Journal of the Association for Information Science and Technology (JASIST) Articles Published Between 2001 and Mid 2017. The 2017 Altmetrics Workshop: The Dependencies of Altmetrics, 26 September 2017, Toronto, Canada. Retrieved from: http://altmetrics.org/altmetrics17/ <br>
- Haustein, S. Grand challenges in altmetrics: heterogeneity, data quality and dependencies. Scientometrics 108, 413–423 (2016). https://doi.org/10.1007/s11192-016-1910-9 <br>
- Poplašen, L. M., & Grgić, I. H. (2017). Altmetric and bibliometric scores: does Open Access matter?. Qualitative and Quantitative Methods in Libraries, 5(2), 451-460. <br>
- Priem, J., Piwowar, H. A., & Hemminger, B. M. (2012). Altmetrics in the wild: Using social media to explore scholarly impact. arXiv preprint arXiv:1203.4745. <br>
- Wouters, P., Zahedi, Z., & Costas, R. (2019). Social media metrics for new research evaluation. In Springer handbook of science and technology indicators (pp. 687-713). Springer, Cham. <br>
- Zahedi Z, Costas R (2018) General discussion of data quality challenges in social media metrics: Extensive comparison of four major altmetric data aggregators. PLoS ONE 13(5): e0197326. https://doi.org/10.1371/journal.pone.0197326 <br>
- Zahedi, Z. (2018). Understanding the value of social media metrics for research evaluation. Doctoral dissertation, Leiden University. <br>


## Related metrics
none <br>

## References
1. Priem, J., Piwowar, H. A., & Hemminger, B. M. (2012). Altmetrics in the wild: Using social media to explore scholarly impact. [https://arxiv.org/abs/1203.4745v1](https://arxiv.org/abs/1203.4745v1)
2. Wouters, P., Zahedi, Z., & Costas, R. (2019). Social Media Metrics for New Research Evaluation. In W. Glänzel, H. F. Moed, U. Schmoch, & M. Thelwall (Eds.), *Springer Handbook of Science and Technology Indicators* (pp. 687–713). Springer International Publishing. https://doi.org/10.1007/978-3-030-02511-3_26
Open Access: https://arxiv.org/ftp/arxiv/papers/1806/1806.10541.pdf
4. Adie, 2013
5. Haustein, 2016; 
6. Zahedi, 2018
7. Bar-Ilan & Halevi, 2017
8. Zahedi & Costas, 2018


Last updated June 2022
